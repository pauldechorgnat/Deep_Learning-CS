{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = {w:i for i, w in  enumerate(self.word2vec.keys())}\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array([vec for vec in self.word2vec.values()], ndmin = 2)\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        if w not in self.word2id.keys():\n",
    "            print(\"The word '{}' is not contained into the vocabulary\".format(w))\n",
    "            return False\n",
    "        else :\n",
    "            \n",
    "            vector = self.word2vec[w]\n",
    "            similarities = vector.T * self.embeddings / np.linalg.norm(vector)/np.sqrt(np.sum(self.embeddings**2, axis = 0))\n",
    "            indexes = np.argsort(np.sum(similarities, axis = 1))[-K:]\n",
    "            \n",
    "            return [self.id2word[index] for index in indexes]\n",
    "\n",
    "    def score(self, w1, w2, display = True):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        try :\n",
    "            vector1 = self.word2vec[w1]\n",
    "            vector2 = self.word2vec[w2]\n",
    "        except KeyError as e:\n",
    "            print(\"The word '{}' is not contained into the vocabulary.\".format(e.args[0]))\n",
    "            return False\n",
    "\n",
    "        return np.dot(vector1, vector2)/np.linalg.norm(vector1)/np.linalg.norm(vector2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 49999 pretrained word vectors\n",
      "cat dog 0.6380517245741391\n",
      "dog pet 0.5802521571141541\n",
      "dogs cats 0.6474995466659008\n",
      "paris france 0.6153761202499448\n",
      "germany berlin 0.591637835087271\n",
      "['kitten', 'feline', 'timezone', 'cats', 'cat']\n",
      "['dogs', 'lr/lc', 'spaniel', 'terrier', 'dog']\n",
      "['breeds', 'dog', 'spaniel', 'terrier', 'dogs']\n",
      "['montparnasse', 'm\\\\xc3\\\\xa9tro', 'military_decorations', 'paris', 'ordre_de_la_lib\\\\xc3\\\\xa3\\\\xc2\\\\xa9ration']\n",
      "['westphalia', 'saxony', 'landkreis', 'vorpommern', 'germany']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'english.vec'), nmax=50000)#Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=25000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "                \n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # mean of word vectors\n",
    "                embedded_sentence = [self.w2v.word2vec[word]\n",
    "                                     if word in self.w2v.word2vec.keys() else np.zeros((300,))\n",
    "                                     for word in sent]\n",
    "                sentemb += [np.mean(embedded_sentence, axis = 0)]\n",
    "                \n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                embedded_sentence = [self.w2v.word2vec[word]*idf[word]\n",
    "                                     if word in self.w2v.word2vec.keys() else np.zeros((300,))\n",
    "                                     for word in sent]\n",
    "                sentemb += [np.mean(embedded_sentence, axis = 0)]\n",
    "                \n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        keys = self.encode(sentences, idf)\n",
    "        query = self.encode([s], idf)\n",
    "        \n",
    "        similarity_vec = query * keys/np.linalg.norm(query)/np.sqrt(np.sum(keys**2, axis = 0))\n",
    "        indexes = np.argsort(np.sum(similarity_vec, axis = 1))[-K:]\n",
    "        print(\"{} most similar sentences :\\n - target sentence : '{}'\".format(K, \" \".join(s)))\n",
    "        for index in indexes : \n",
    "            print(\" - '\" + \" \".join(sentences[index])+\"'\")\n",
    "        return [sentences[index] for index in indexes]\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        s1_encoded = self.encode([s1], idf)\n",
    "        s2_encoded = self.encode([s2], idf)\n",
    "        similarity = np.sum(s1_encoded* s2_encoded)/np.linalg.norm(s1_encoded)/np.linalg.norm(s2_encoded)\n",
    "        \n",
    "        print(\"Similarity between :\\n - sentence 1: '{}'\\n - sentence 2: '{}'\\n => {}\".format(' '.join(s1),\n",
    "                                                                                          ' '.join(s2),\n",
    "                                                                                          similarity))\n",
    "        return similarity\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "        for sent in sentences:\n",
    "            for w in set(sent):\n",
    "                idf[w] = idf.get(w, 0) + 1\n",
    "        \n",
    "        for word in idf.keys():\n",
    "            idf[word] = max(1, np.log10(len(sentences) / (idf[word])))\n",
    "        return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences(path):\n",
    "    sentences = []\n",
    "    with open(path, 'r', encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            sentences += [line.split(' ')[:-1]]# we get rid of the \\n\n",
    "    file.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 pretrained word vectors\n",
      "5 most similar sentences :\n",
      " - target sentence : '1 smiling african american boy .'\n",
      " - 'a boy jumps on another boy .'\n",
      " - 'a boy skateboarding'\n",
      " - 'teen boy playing billiards .'\n",
      " - 'boy riding a horse .'\n",
      " - 'boy plays baseball .'\n",
      "Similarity between :\n",
      " - sentence 1: '1 man singing and 1 man playing a saxophone in a concert .'\n",
      " - sentence 2: '10 people venture out to go crosscountry skiing .'\n",
      " => 0.6089445116147134\n",
      "5 most similar sentences :\n",
      " - target sentence : '1 smiling african american boy .'\n",
      " - '4 people play soccer'\n",
      " - 'a small boy following 4 geese .'\n",
      " - 'a train station platform at 11 : 27 in the morning .'\n",
      " - '1 smiling african american boy .'\n",
      " - 'horse number 8 is racing'\n",
      "Similarity between :\n",
      " - sentence 1: '1 man singing and 1 man playing a saxophone in a concert .'\n",
      " - sentence 2: '10 people venture out to go crosscountry skiing .'\n",
      " => 0.5963335652885747\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=5000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "sentences = load_sentences(os.path.join(PATH_TO_DATA, \"sentences.txt\"))\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = {} if False else s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13])\n",
    "\n",
    "\n",
    "#idf = {}  \n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "_ = s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Download and load 50k first vectors of fasttext data for french and english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "english_vec_url = \"https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\"\n",
    "french_vec_url = \"https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\"\n",
    "\n",
    "def load_data_vec(url, max_count, output_file):\n",
    "    r = requests.get(url, stream = True)\n",
    "    data = {}\n",
    "    counter =0\n",
    "    with open(output_file, 'w', encoding = \"utf-8\") as output_file:\n",
    "        for line in r.iter_lines():\n",
    "            if counter >= max_count:\n",
    "                break\n",
    "            splits = str(line).split(\" \")\n",
    "            if len(splits) == 302:\n",
    "                counter += 1\n",
    "                line = str(line).replace(\"b'\", \"\").replace(\"'b\", \"\")\n",
    "            \n",
    "                output_file.write(str(line)+ \"\\n\")\n",
    "    output_file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = False\n",
    "if download : french_vec = load_data_vec(french_vec_url, 50000, os.path.join(PATH_TO_DATA, \"french.vec\"))\n",
    "if download : english_vec = load_data_vec(english_vec_url, 50000, os.path.join(PATH_TO_DATA, \"english.vec\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "     Use it to create the matrix X and Y (of aligned embeddings for these words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 49999 pretrained word vectors\n",
      "Loaded 49999 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "w2v_fr = Word2vec(os.path.join(PATH_TO_DATA, 'french.vec'), nmax=50000)\n",
    "w2v_en = Word2vec(os.path.join(PATH_TO_DATA, 'english.vec'), nmax=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_words = w2v_fr.word2id\n",
    "english_words = w2v_en.word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_words = set(french_words.keys()).intersection(set(english_words.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18968"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "     Use it to create the matrix X and Y (of aligned embeddings for these words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([w2v_fr.word2vec[word_fr] for word_fr in common_words]).T\n",
    "Y = np.array([w2v_en.word2vec[word_en] for word_en in common_words]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 18968)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.dot(Y, X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, sigma, V = np.linalg.svd(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.dot(U,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "     You will be evaluated on that part and the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_translation(word_to_translate, translation_matrix,\n",
    "                             destination_w2v, starting_w2v, nearest_neighbours = 5):\n",
    "    # embedding the english word\n",
    "    vector_start = starting_w2v.word2vec[word_to_translate].reshape(-1, )\n",
    "    # translating the embedding\n",
    "    vector_dest = np.dot(translation_matrix.T, vector_start).reshape(-1,)\n",
    "    # looking for the closest vectors in french\n",
    "    similarities = np.sum(vector_dest.T * destination_w2v.embeddings / np.linalg.norm(vector_dest)/np.sqrt(np.sum(destination_w2v.embeddings**2, axis = 0)), axis = 1)\n",
    "\n",
    "    indexes = np.argsort(similarities)[-nearest_neighbours:]\n",
    "    \n",
    "    print(\"The nearest {} neighbours of '{}' are : \".format(nearest_neighbours, word_to_translate))\n",
    "    for index, index_vec in enumerate(indexes):\n",
    "        print(\"{} - '{}'\".format(nearest_neighbours-index, destination_w2v.id2word[index_vec]))\n",
    "    #return [destination_w2v.id2word[index] for index in indexes]\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nearest 10 neighbours of 'cat' are : \n",
      "10 - 'hound'\n",
      "9 - 'dog'\n",
      "8 - 'tail'\n",
      "7 - 'mammif\\xc3\\xa8re'\n",
      "6 - 'cats'\n",
      "5 - 'grizzly'\n",
      "4 - 'chat'\n",
      "3 - 'felis'\n",
      "2 - 'canis'\n",
      "1 - 'cat'\n",
      "The nearest 10 neighbours of 'dog' are : \n",
      "10 - 'dogs'\n",
      "9 - 'canis'\n",
      "8 - 'poney'\n",
      "7 - 'sheep'\n",
      "6 - 'porky'\n",
      "5 - 'pig'\n",
      "4 - 'dingo'\n",
      "3 - 'chien'\n",
      "2 - 'dog'\n",
      "1 - 'hound'\n",
      "The nearest 10 neighbours of 'boy' are : \n",
      "10 - 'adventures'\n",
      "9 - 'teenage'\n",
      "8 - 'boys'\n",
      "7 - 'wife'\n",
      "6 - 'boyz'\n",
      "5 - 'daughter'\n",
      "4 - 'dogg'\n",
      "3 - 'sailor'\n",
      "2 - 'girl'\n",
      "1 - 'boy'\n",
      "The nearest 10 neighbours of 'girl' are : \n",
      "10 - 'm\\xc3\\xa4dchen'\n",
      "9 - 'teenage'\n",
      "8 - '\\xe2\\x94\\x9c\\xe2\\x94\\x80'\n",
      "7 - 'musume'\n",
      "6 - 'girlfriend'\n",
      "5 - 'actress'\n",
      "4 - 'woman'\n",
      "3 - 'sailor'\n",
      "2 - 'girls'\n",
      "1 - 'girl'\n",
      "The nearest 10 neighbours of 'car' are : \n",
      "10 - 'audi'\n",
      "9 - 'peugeot'\n",
      "8 - 'pilote_record_tour'\n",
      "7 - 'wrc'\n",
      "6 - 'roadster'\n",
      "5 - 'motorsport'\n",
      "4 - 'wtcc'\n",
      "3 - 'porsche'\n",
      "2 - '\\xc3\\xa9curie_record_tour'\n",
      "1 - '\\xc3\\xa9curie_pole'\n",
      "The nearest 10 neighbours of 'computer' are : \n",
      "10 - 'windows'\n",
      "9 - 'amiga'\n",
      "8 - 'informatique'\n",
      "7 - 'microprocesseur'\n",
      "6 - 'amstrad'\n",
      "5 - 'unix'\n",
      "4 - 'intel'\n",
      "3 - 'computing'\n",
      "2 - 'graphics'\n",
      "1 - 'computer'\n",
      "The nearest 10 neighbours of 'king' are : \n",
      "10 - 'queen'\n",
      "9 - 'emperor'\n",
      "8 - 'kingdom'\n",
      "7 - 'wessex'\n",
      "6 - 'tr\\xc3\\xb4ne'\n",
      "5 - 'consort'\n",
      "4 - 'rois'\n",
      "3 - 'king'\n",
      "2 - 'roi'\n",
      "1 - '\\xe2\\x94\\x9c\\xe2\\x94\\x80'\n",
      "The nearest 10 neighbours of 'natural' are : \n",
      "10 - 'araneae'\n",
      "9 - 'biological'\n",
      "8 - 'amphibia'\n",
      "7 - 'reptilia'\n",
      "6 - 'batrachians'\n",
      "5 - 'anura'\n",
      "4 - 'herpetology'\n",
      "3 - 'zoological'\n",
      "2 - 'zoology'\n",
      "1 - 'natural'\n",
      "The nearest 10 neighbours of 'language' are : \n",
      "10 - 'langues'\n",
      "9 - 'dialecte'\n",
      "8 - 'phonologie'\n",
      "7 - 'parl\\xc3\\xa9es'\n",
      "6 - 'langue'\n",
      "5 - 'parl\\xc3\\xa9e'\n",
      "4 - 'linguistics'\n",
      "3 - 'linguistic'\n",
      "2 - 'language'\n",
      "1 - 'languages'\n",
      "The nearest 10 neighbours of 'processing' are : \n",
      "10 - '&c_nivgeo'\n",
      "9 - '&nivgeo'\n",
      "8 - 'darkgrey'\n",
      "7 - 'fr/cassini/fr/html/fiche'\n",
      "6 - 'select_resultat'\n",
      "5 - 'processing'\n",
      "4 - '&annee'\n",
      "3 - 'fr/fr/methodes/zonages/au'\n",
      "2 - '\\xe2\\x94\\x9c\\xe2\\x94\\x80'\n",
      "1 - 'autres_projets'\n"
     ]
    }
   ],
   "source": [
    "test_words_en = ['cat', 'dog', 'boy', 'girl', 'car', 'computer', 'king', 'natural', 'language', 'processing']\n",
    "for eng_word in test_words_en:\n",
    "    _ = find_closest_translation(eng_word, W, w2v_fr, w2v_en, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nearest 10 neighbours of 'chat' are : \n",
      "10 - 'bellied'\n",
      "9 - 'rat'\n",
      "8 - 'raccoons'\n",
      "7 - 'timezone'\n",
      "6 - 'hamster'\n",
      "5 - 'lemur'\n",
      "4 - 'squirrel'\n",
      "3 - 'vole'\n",
      "2 - 'macaque'\n",
      "1 - 'lr/lc'\n",
      "The nearest 10 neighbours of 'chien' are : \n",
      "10 - 'breeds'\n",
      "9 - 'canis'\n",
      "8 - 'kennel'\n",
      "7 - 'fci'\n",
      "6 - 'dog'\n",
      "5 - 'poodle'\n",
      "4 - 'vole'\n",
      "3 - 'spaniel'\n",
      "2 - 'terrier'\n",
      "1 - 'lr/lc'\n",
      "The nearest 10 neighbours of 'homme' are : \n",
      "10 - 'anatomist'\n",
      "9 - 'industrialist'\n",
      "8 - '_at_tn'\n",
      "7 - 'politiques'\n",
      "6 - 'businessman'\n",
      "5 - 'politique'\n",
      "4 - 'latns'\n",
      "3 - 'diplomat'\n",
      "2 - 'homme'\n",
      "1 - 'politician'\n",
      "The nearest 10 neighbours of 'fille' are : \n",
      "10 - 'married'\n",
      "9 - 'eldest'\n",
      "8 - 'predeceased'\n",
      "7 - '\\xe2\\x94\\x82'\n",
      "6 - 'granddaughter'\n",
      "5 - 'n\\xc3\\xa9e'\n",
      "4 - 'remarried'\n",
      "3 - 'daughter'\n",
      "2 - 'heiress'\n",
      "1 - 'noblewoman'\n",
      "The nearest 10 neighbours of 'voiture' are : \n",
      "10 - 'sedans'\n",
      "9 - 'porsche'\n",
      "8 - 'coup\\xc3\\xa9'\n",
      "7 - 'car'\n",
      "6 - 'hatchback'\n",
      "5 - 'chevrolet'\n",
      "4 - 'wheelbase'\n",
      "3 - 'citro\\xc3\\xabn'\n",
      "2 - 'suv'\n",
      "1 - 'peugeot'\n",
      "The nearest 10 neighbours of 'ordinateur' are : \n",
      "10 - 'system/'\n",
      "9 - 'photo/scan'\n",
      "8 - 'vga'\n",
      "7 - 'powerpc'\n",
      "6 - 'tcp/ip'\n",
      "5 - 'usb'\n",
      "4 - 'firmware'\n",
      "3 - 'i/o'\n",
      "2 - 'scsi'\n",
      "1 - 'cp/m'\n",
      "The nearest 10 neighbours of 'roi' are : \n",
      "10 - 'abdicated'\n",
      "9 - 'kingship'\n",
      "8 - 'throne'\n",
      "7 - 'pertuan'\n",
      "6 - 'anjou'\n",
      "5 - 'lusignan'\n",
      "4 - '}+}+}+}+'\n",
      "3 - 'king'\n",
      "2 - 'm\\xc3\\xa1el'\n",
      "1 - 'vassal'\n",
      "The nearest 10 neighbours of 'traitement' are : \n",
      "10 - 'jp/sfms'\n",
      "9 - 'benzodiazepines'\n",
      "8 - 'north_carolina'\n",
      "7 - 'latm'\n",
      "6 - 'intravenous'\n",
      "5 - 'latns'\n",
      "4 - 'canals_in_south_carolina'\n",
      "3 - '}+}+}+}+'\n",
      "2 - 'world_war_ii'\n",
      "1 - 'world_war_i'\n",
      "The nearest 10 neighbours of 'automatique' are : \n",
      "10 - 'next_type'\n",
      "9 - 'jp/sfms'\n",
      "8 - 'edit&preload'\n",
      "7 - 'automated'\n",
      "6 - 'military_equipment_by_type'\n",
      "5 - 'r\\xc3\\xaeurile'\n",
      "4 - 'whitelisted'\n",
      "3 - 'ateahouse%'\n",
      "2 - '}+}+}+}+'\n",
      "1 - 'automatic'\n",
      "The nearest 10 neighbours of 'langage' are : \n",
      "10 - 'grammars'\n",
      "9 - 'unix'\n",
      "8 - 'xml'\n",
      "7 - 'c#'\n",
      "6 - 'syntactic'\n",
      "5 - 'parser'\n",
      "4 - 'posix'\n",
      "3 - 'lisp'\n",
      "2 - 'c++'\n",
      "1 - 'sql'\n",
      "The nearest 10 neighbours of 'automatique' are : \n",
      "10 - 'next_type'\n",
      "9 - 'jp/sfms'\n",
      "8 - 'edit&preload'\n",
      "7 - 'automated'\n",
      "6 - 'military_equipment_by_type'\n",
      "5 - 'r\\xc3\\xaeurile'\n",
      "4 - 'whitelisted'\n",
      "3 - 'ateahouse%'\n",
      "2 - '}+}+}+}+'\n",
      "1 - 'automatic'\n"
     ]
    }
   ],
   "source": [
    "test_words_fr = ['chat', 'chien', 'homme', 'fille', 'voiture', 'ordinateur', 'roi', 'traitement', 'automatique', 'langage', 'automatique']\n",
    "for fre_word in test_words_fr:\n",
    "    _ = find_closest_translation(fre_word, W.T, w2v_en, w2v_fr, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "    (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SST_PATH = os.path.join(PATH_TO_DATA, \"SST\")\n",
    "sentences_train = load_sentences(path = os.path.join(SST_PATH, \"stsa.fine.train\"))\n",
    "sentences_dev = load_sentences(path = os.path.join(SST_PATH, \"stsa.fine.dev\"))\n",
    "sentences_test = load_sentences(path = os.path.join(SST_PATH, \"stsa.fine.test.X\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Encode sentences with the BoV model above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will consider the w2V model built at the previous question\n",
    "bov = BoV(w2v_en)\n",
    "idf_train = bov.build_idf(sentences_train)\n",
    "idf_dev = bov.build_idf(sentences_dev)\n",
    "idf_test = bov.build_idf(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_train = bov.encode(sentences_train, idf_train)\n",
    "emb_dev = bov.encode(sentences_dev, idf_dev)\n",
    "emb_test = bov.encode(sentences_test, idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_train = np.array(emb_train, ndmin=2)\n",
    "emb_dev = np.array(emb_dev, ndmin=2)\n",
    "emb_test = np.array(emb_test, ndmin=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "     (consider tuning the L2 regularization on the dev set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = np.array([int(sentence[0]) for sentence in sentences_train], ndmin=1)\n",
    "labels_dev = np.array([int(sentence[0]) for sentence in sentences_dev], ndmin=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "target_train = pd.get_dummies(labels_train).values\n",
    "target_dev = pd.get_dummies(labels_dev).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(LogisticRegression(C=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(emb_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(emb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score : 0.46301498127340823\n"
     ]
    }
   ],
   "source": [
    "print(\"Training score : {}\".format(sum(pred_train == labels_train)/len(pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "details on training data\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.25      0.34      1092\n",
      "          1       0.44      0.62      0.52      2218\n",
      "          2       0.44      0.21      0.28      1624\n",
      "          3       0.45      0.62      0.52      2322\n",
      "          4       0.58      0.41      0.48      1288\n",
      "\n",
      "avg / total       0.47      0.46      0.44      8544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"details on training data\")\n",
    "print(classification_report(labels_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dev = model.predict(emb_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "details on development data\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.37      0.20      0.26       139\n",
      "          1       0.40      0.56      0.47       289\n",
      "          2       0.27      0.10      0.14       229\n",
      "          3       0.36      0.56      0.44       279\n",
      "          4       0.45      0.30      0.36       165\n",
      "\n",
      "avg / total       0.37      0.38      0.35      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"details on development data\")\n",
    "print(classification_report(labels_dev, pred_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score : 0.3787465940054496\n"
     ]
    }
   ],
   "source": [
    "print(\"Training score : {}\".format(sum(pred_dev == labels_dev)/len(pred_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "     You will be evaluated on the results of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(emb_test)\n",
    "np.savetxt(fname = os.path.join(PATH_TO_DATA, \"logreg_bov_y_test_sst.txt\"), X = pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 - Try to improve performance with another classifier\n",
    "     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(50, input_shape = (300, ), activation = 'relu'))\n",
    "nn_model.add(Dropout(.3))\n",
    "nn_model.add(Dense(30, activation = \"relu\"))\n",
    "nn_model.add(Dense(5, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdac35d6e80>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.compile(loss = \"categorical_crossentropy\", metrics = ['accuracy'], optimizer = 'adam')\n",
    "nn_model.fit(x = emb_train, \n",
    "             y=target_train, \n",
    "             validation_data=(emb_dev, target_dev), \n",
    "             batch_size = 32,\n",
    "             epochs = 50, \n",
    "             verbose = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1101/1101 [==============================] - 0s 75us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6138260589741664, 0.3678474114576759]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.evaluate(emb_dev, target_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Load train/dev/test sets of SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_sentences(path):\n",
    "    sentences = []\n",
    "    with open(path, 'r', encoding = 'utf-8') as input_file:\n",
    "        for line in input_file:\n",
    "            sentences += [str(line).replace(\"'b\", \"\")\n",
    "                          .replace(\"\\n\", \"\")[2:]]\n",
    "    input_file.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_sentences = load_raw_sentences(os.path.join(SST_PATH, \"stsa.fine.train\"))\n",
    "raw_dev_sentences = load_raw_sentences(os.path.join(SST_PATH, \"stsa.fine.dev\"))\n",
    "raw_test_sentences = load_raw_sentences(os.path.join(SST_PATH, \"stsa.fine.test.X\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "     https://keras.io/preprocessing/text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocabulary_size = len(set([word for sent in raw_train_sentences for word in sent.split(\" \")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_sentences = [one_hot(sentence, round(train_vocabulary_size*1.3)) for sentence in raw_train_sentences]\n",
    "encoded_dev_sentences = [one_hot(sentence, round(train_vocabulary_size*1.3)) for sentence in raw_dev_sentences]\n",
    "encoded_test_sentences = [one_hot(sentence, round(train_vocabulary_size*1.3)) for sentence in raw_test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "     https://keras.io/preprocessing/sequence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_encoded_train_sentences = pad_sequences(encoded_train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_train = padded_encoded_train_sentences.shape[1]\n",
    "padded_encoded_dev_sentences = pad_sequences(encoded_dev_sentences, max_len_train)\n",
    "padded_encoded_test_sentences = pad_sequences(encoded_test_sentences, max_len_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Design your encoder + classifier using keras.layers\n",
    "     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "     All of these components are contained in the Sequential() and are trained together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ADAPT CODE BELOW\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "K.clear_session()\n",
    "\n",
    "embed_dim  = 300  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = round(train_vocabulary_size*1.3)  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 - Define your loss/optimizer/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         6466500   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 6,560,265\n",
      "Trainable params: 6,560,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'rmsprop' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 - Train your model and find the best hyperparameters for your dev set\n",
    "     you will be evaluated on the quality of your predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = padded_encoded_train_sentences\n",
    "x_val = padded_encoded_dev_sentences\n",
    "y_train = target_train\n",
    "y_val = target_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/6\n",
      "8544/8544 [==============================] - 66s 8ms/step - loss: 1.5545 - acc: 0.2972 - val_loss: 1.4871 - val_acc: 0.3470\n",
      "Epoch 2/6\n",
      "8544/8544 [==============================] - 65s 8ms/step - loss: 1.3692 - acc: 0.3991 - val_loss: 1.3696 - val_acc: 0.3851\n",
      "Epoch 3/6\n",
      "8544/8544 [==============================] - 65s 8ms/step - loss: 1.1920 - acc: 0.4529 - val_loss: 1.4192 - val_acc: 0.3751\n",
      "Epoch 4/6\n",
      "8544/8544 [==============================] - 134s 16ms/step - loss: 1.0471 - acc: 0.5123 - val_loss: 1.5350 - val_acc: 0.3842\n",
      "Epoch 5/6\n",
      "8544/8544 [==============================] - 813s 95ms/step - loss: 0.9218 - acc: 0.6136 - val_loss: 1.6095 - val_acc: 0.3878\n",
      "Epoch 6/6\n",
      "8544/8544 [==============================] - 212s 25ms/step - loss: 0.7938 - acc: 0.7001 - val_loss: 1.6612 - val_acc: 0.3688\n"
     ]
    }
   ],
   "source": [
    "# ADAPT CODE BELOW\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, epochs=n_epochs, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
